{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load [a9a](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a9a) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_svmlight_files(['a9a', 'a9a.t'])\n",
    "y_train = np.where(y_train > 0, y_train, y_train + 1)\n",
    "y_test = np.where(y_test > 0, y_test, y_test + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch_sparse_tensor(M):\n",
    "    M = M.tocoo().astype(np.float32)\n",
    "    i = torch.from_numpy(np.vstack((M.row, M.col))).long()\n",
    "    v = torch.from_numpy(M.data)\n",
    "    shape = torch.Size(M.shape)\n",
    "    return torch.sparse.FloatTensor(i, v, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.add_module('linear', torch.nn.Linear(x_train.shape[1], 1))\n",
    "model.add_module('sigmoid', torch.nn.Sigmoid())\n",
    "optimizer = optim.Adagrad(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "batch_size = 100\n",
    "train_size = x_train.shape[0]\n",
    "\n",
    "def train(model):\n",
    "    x, y = shuffle(x_train, y_train, random_state=1)\n",
    "\n",
    "    def get_batch():\n",
    "        i = 0\n",
    "        for i in range(0, train_size // batch_size):\n",
    "            yield Variable(to_torch_sparse_tensor(x[i:i+batch_size]).to_dense()), Variable(torch.from_numpy(y[i:i+batch_size]).float())\n",
    "\n",
    "    for vx, vy in get_batch():\n",
    "        optimizer.zero_grad()\n",
    "        fx = model.forward(vx)\n",
    "        output = F.binary_cross_entropy(fx, vy)\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(output.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a14518/venvs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29141736030578613\n",
      "0.2592703104019165\n",
      "0.24327896535396576\n",
      "0.23324090242385864\n",
      "0.22619690001010895\n",
      "0.2209147810935974\n",
      "0.2167748659849167\n",
      "0.21342603862285614\n",
      "0.21065229177474976\n",
      "0.20831218361854553\n",
      "0.20630861818790436\n",
      "0.20457252860069275\n",
      "0.2030528485774994\n",
      "0.20171146094799042\n",
      "0.2005186825990677\n",
      "0.1994512975215912\n",
      "0.19849087297916412\n",
      "0.19762231409549713\n",
      "0.19683349132537842\n",
      "0.19611425697803497\n",
      "0.19545617699623108\n",
      "0.19485199451446533\n",
      "0.19429579377174377\n",
      "0.19378234446048737\n",
      "0.19330722093582153\n",
      "0.19286659359931946\n",
      "0.19245702028274536\n",
      "0.1920756697654724\n",
      "0.1917198896408081\n",
      "0.19138753414154053\n",
      "0.19107647240161896\n",
      "0.19078484177589417\n",
      "0.19051134586334229\n",
      "0.19025419652462006\n",
      "0.1900123804807663\n",
      "0.18978460133075714\n",
      "0.18956995010375977\n",
      "0.18936733901500702\n",
      "0.18917599320411682\n",
      "0.18899506330490112\n",
      "0.1888239085674286\n",
      "0.18866193294525146\n",
      "0.18850842118263245\n",
      "0.18836286664009094\n",
      "0.1882248818874359\n",
      "0.18809381127357483\n",
      "0.18796932697296143\n",
      "0.18785108625888824\n",
      "0.18773871660232544\n",
      "0.18763186037540436\n",
      "0.18753017485141754\n",
      "0.1874333769083023\n",
      "0.18734121322631836\n",
      "0.1872534602880478\n",
      "0.1871698498725891\n",
      "0.1870901882648468\n",
      "0.18701431155204773\n",
      "0.18694187700748444\n",
      "0.1868729144334793\n",
      "0.18680711090564728\n",
      "0.18674436211585999\n",
      "0.1866844892501831\n",
      "0.18662747740745544\n",
      "0.1865730732679367\n",
      "0.18652111291885376\n",
      "0.1864715814590454\n",
      "0.18642432987689972\n",
      "0.1863793134689331\n",
      "0.18633633852005005\n",
      "0.18629536032676697\n",
      "0.18625633418560028\n",
      "0.18621903657913208\n",
      "0.1861836016178131\n",
      "0.18614979088306427\n",
      "0.18611766397953033\n",
      "0.18608704209327698\n",
      "0.1860579252243042\n",
      "0.18603020906448364\n",
      "0.1860038936138153\n",
      "0.18597880005836487\n",
      "0.18595504760742188\n",
      "0.18593242764472961\n",
      "0.18591105937957764\n",
      "0.18589076399803162\n",
      "0.18587154150009155\n",
      "0.18585346639156342\n",
      "0.18583619594573975\n",
      "0.18582002818584442\n",
      "0.18580470979213715\n",
      "0.18579038977622986\n",
      "0.18577678501605988\n",
      "0.1857641041278839\n",
      "0.1857522428035736\n",
      "0.18574108183383942\n",
      "0.1857307404279709\n",
      "0.18572109937667847\n",
      "0.1857120841741562\n",
      "0.185703843832016\n",
      "0.18569624423980713\n",
      "0.18568916618824005\n"
     ]
    }
   ],
   "source": [
    "for _ in range(0, 100):\n",
    "    train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86449031584102132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, model(Variable(to_torch_sparse_tensor(x_test).to_dense())).data.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
